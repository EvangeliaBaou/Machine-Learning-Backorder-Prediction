{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final-Backorder Prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxFjdz1kQffQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from numpy import unique\r\n",
        "from imblearn.under_sampling import RandomUnderSampler\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from imblearn.pipeline import Pipeline\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn import metrics\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.model_selection import cross_val_predict\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "# import warnings filter\r\n",
        "from warnings import simplefilter\r\n",
        "# ignore all future warnings\r\n",
        "simplefilter(action='ignore', category=FutureWarning)\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import validation_curve\r\n",
        "import mlxtend\r\n",
        "import pandas as pd\r\n",
        "from sklearn.metrics import precision_recall_curve\r\n",
        "from sklearn.metrics import plot_precision_recall_curve\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuPovjCAQnWb"
      },
      "source": [
        "df = pd.read_csv('backorder prediction.csv') #load dataset\r\n",
        "#map binary variables with 'Yes' as 1 and 'No' as 0 \r\n",
        "df['stop_auto_buy']= df['stop_auto_buy'].map({'Yes':1, 'No':0})\r\n",
        "df['potential_issue']= df['potential_issue'].map({'Yes':1, 'No':0})\r\n",
        "df['oe_constraint']= df['oe_constraint'].map({'Yes':1, 'No':0})\r\n",
        "df['ppap_risk']= df['ppap_risk'].map({'Yes':1, 'No':0})\r\n",
        "df['deck_risk']= df['deck_risk'].map({'Yes':1, 'No':0})\r\n",
        "df['rev_stop']= df['rev_stop'].map({'Yes':1, 'No':0})\r\n",
        "df['went_on_backorder']= df['went_on_backorder'].map({'Yes':1, 'No':0})\r\n",
        "df = df.dropna(how='any',axis=0)  #delete nulls\r\n",
        "\r\n",
        "# pie chart of percentage of two classes\r\n",
        "b = df['went_on_backorder'].value_counts()\r\n",
        "total_values = []\r\n",
        "went_on_backorder = ['Class:0', 'Class:1']\r\n",
        "for n in b:\r\n",
        "    total_values.append(n)\r\n",
        "plt.pie(total_values, labels=went_on_backorder, autopct='%0.f%%', shadow=True,\r\n",
        "        startangle=90)  \r\n",
        "plt.show()\r\n",
        "# Input_ y_Target_Variable. \r\n",
        "y = df['went_on_backorder']\r\n",
        "y=y.to_numpy()\r\n",
        "\r\n",
        "#plot correlation matrix (pearson)\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(13,13))\r\n",
        "corr = df.corr()\r\n",
        "sns.heatmap(corr, vmax=1.0, center=0, fmt='.2f',\r\n",
        "                 square=True, linewidths=.6, annot=True, cbar_kws={\"shrink\": .60})\r\n",
        "\r\n",
        "print('Correlation Matrix (Pearsons Correlation)' )\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#plot correlation matrix (Spearman's)\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(13,13))\r\n",
        "corr = df.corr(method='spearman')\r\n",
        "sns.heatmap(corr, vmax=1.0, center=0, fmt='.2f',\r\n",
        "                 square=True, linewidths=.6, annot=True, cbar_kws={\"shrink\": .60})\r\n",
        "print('Correlation Matrix (Spearman Correlation)' )\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#print percentage per class\r\n",
        "classes = unique(y)\r\n",
        "total = len(y)\r\n",
        "for c in classes:\r\n",
        "\tn_examples = len(y[y==c])\r\n",
        "\tpercent = n_examples / total * 100\r\n",
        "\tprint('> Class=%d: %d/%d (%.1f%%)' % (c, n_examples, total, percent))\r\n",
        "\r\n",
        "del df['went_on_backorder'] #delete target col from df\r\n",
        "\r\n",
        "\r\n",
        "df[df['national_inv'] < 0] = 0 #set negative values of feture egual to zero\r\n",
        "df[df['perf_6_month_avg'] < 0] = np.NaN #set negative values of feture as NaN\r\n",
        "df[df['perf_12_month_avg'] < 0] = np.NaN #set negative values of feture as NaN\r\n",
        "\r\n",
        "# Input_x_Features. \r\n",
        "X= df.to_numpy(dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URY4cJkthbCo"
      },
      "source": [
        "#split dataset in train and test\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "# summarize datasets\r\n",
        "classes = unique(y)\r\n",
        "total = len(y)\r\n",
        "classes_y_train = unique(y_train)\r\n",
        "total_y_train = len(y_train)\r\n",
        "imputation=SimpleImputer(strategy='mean')\r\n",
        "scaling=StandardScaler()\r\n",
        "X_train=imputation.fit_transform(X_train)\r\n",
        "X_train=scaling.fit_transform(X_train)\r\n",
        "sm=SMOTE(sampling_strategy=0.4)\r\n",
        "X_train,y_train=sm.fit_resample(X_train,y_train)\r\n",
        "X_test=imputation.transform(X_test)\r\n",
        "X_test=scaling.transform(X_test)\r\n",
        "for c in classes_y_train:\r\n",
        "\tn_examples = len(y_train[y_train==c])\r\n",
        "\tpercent = n_examples / total_y_train * 100\r\n",
        "\tprint('> Class=%d in train: %d/%d (%.1f%%)' % (c, n_examples, total_y_train, percent))\r\n",
        "classes_y_test = unique(y_test)\r\n",
        "total_y_test = len(y_test)\r\n",
        "for c in classes_y_test:\r\n",
        "\tn_examples = len(y_test[y_test==c])\r\n",
        "\tpercent = n_examples / total_y_test * 100\r\n",
        "\tprint('> Class=%d in test: %d/%d (%.1f%%)' % (c, n_examples, total_y_test, percent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPgqCsPMQ0-P"
      },
      "source": [
        "model=SVC()\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_svm = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('-----------------------------------------')\r\n",
        "print('Prediction results Of SVM')\r\n",
        "print(confusion_matrix(y_test, y_pred_svm))\r\n",
        "print(classification_report(y_test, y_pred_svm))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu9rjUcSULTk"
      },
      "source": [
        "model = RandomForestClassifier()\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_RF = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of Random Forest')\r\n",
        "print(confusion_matrix(y_test, y_pred_RF))\r\n",
        "print(classification_report(y_test, y_pred_RF))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWe8xVmDUKRL"
      },
      "source": [
        "model=KNeighborsClassifier()\r\n",
        "model.fit(X_train,y_train.ravel())\r\n",
        "y_pred_KNN = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of KNN')\r\n",
        "print(confusion_matrix(y_test, y_pred_KNN))\r\n",
        "print(classification_report(y_test, y_pred_KNN))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E72KSuuQZJnw"
      },
      "source": [
        "#outlier detection\r\n",
        "z = np.abs(stats.zscore(X))\r\n",
        "# value greater then 3 is mark as outlier\r\n",
        "threshold = 3\r\n",
        "new=np.where(z>3)\r\n",
        "indices = unique(new[0])\r\n",
        "how_many_classes=np.take(y, indices)\r\n",
        "classes_outliers = unique(how_many_classes)\r\n",
        "total_outliers = len(how_many_classes)\r\n",
        "for c in classes_outliers:\r\n",
        "  n_examples = len(how_many_classes[how_many_classes==c])\r\n",
        "  percent = (n_examples / total_outliers) * 100\r\n",
        "  print('> Class=%d has: %d/%d (%.1f%%) outliers' % (c, n_examples, total_outliers, percent))\r\n",
        "X = np.delete(X, indices, axis=0)\r\n",
        "y=np.delete(y, indices, axis=0)\r\n",
        "classes_rem_outliers = unique(y)\r\n",
        "total = len(y)\r\n",
        "for c in classes_rem_outliers:\r\n",
        "  n_examples = len(y[y==c])\r\n",
        "  percent = (n_examples / total) * 100\r\n",
        "  print('> Class=%d after removing outliers: %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0TDv_BVQ-UT"
      },
      "source": [
        "#Hyparameter tunig for Random Forest\r\n",
        "#split dataset in train and test\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "#define the steps of the pipeline for train-validation dataset\r\n",
        "estimators_RF = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "             ('scale', StandardScaler()),\r\n",
        "             ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "               ('RF', RandomForestClassifier())]\r\n",
        "#define grid params  \r\n",
        "grid_RF = [{'RF__criterion': ['gini','entropy'],\r\n",
        "         'RF__n_estimators': [100,110,120,130],\r\n",
        "         'RF__max_depth':[1,5,10,15,20,25],\r\n",
        "         'RF__max_features':['sqrt','auto']}]\r\n",
        "pipeline_RF = Pipeline(estimators_RF)\r\n",
        "kfold = StratifiedKFold(n_splits=5)\r\n",
        "grid_search_RF = GridSearchCV(pipeline_RF, grid_RF,cv=kfold,scoring='f1')\r\n",
        "grid_search_RF.fit(X_train,y_train.ravel()) \r\n",
        "print('Best parameters for Random Forest')\r\n",
        "print(grid_search_RF.best_params_)\r\n",
        "print('---------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAjpR10mRJnM"
      },
      "source": [
        "#Hyparameter tunig for SVM\r\n",
        "estimators_svm = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "                    ('scale', StandardScaler()),\r\n",
        "                    ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "                   ('svc', SVC())]\r\n",
        "#define grid params  \r\n",
        "grid_svm = [{'svc__kernel': ['rbf'],\r\n",
        "         'svc__gamma': ['auto','scale',0.2],\r\n",
        "         'svc__C':[1,10,20]}]\r\n",
        "pipeline_svm = Pipeline(estimators_svm)\r\n",
        "grid_search_svm = GridSearchCV(pipeline_svm, grid_svm,cv=kfold,scoring='f1')\r\n",
        "grid_search_svm.fit(X_train,y_train.ravel()) \r\n",
        "print('Best parameters for SVM')\r\n",
        "print(grid_search_svm.best_params_)\r\n",
        "print('---------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gEC9jsLzqoj"
      },
      "source": [
        "#Hyparameter tunig for KNN\r\n",
        "estimators_KNN = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "                    ('scale', StandardScaler()),\r\n",
        "                    ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "                   ('KNN', KNeighborsClassifier())]\r\n",
        "#define grid params  \r\n",
        "grid_KNN = [{'KNN__n_neighbors':[2,5,7,10]}]\r\n",
        "pipeline_KNN = Pipeline(estimators_KNN)\r\n",
        "grid_search_KNN = GridSearchCV(pipeline_KNN, grid_KNN,cv=kfold,scoring='f1')\r\n",
        "grid_search_KNN.fit(X_train,y_train.ravel()) \r\n",
        "print('Best parameters for KNN')\r\n",
        "print(grid_search_KNN.best_params_)\r\n",
        "print('---------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47E5_zzhCJa"
      },
      "source": [
        "#split dataset in train and test\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "# summarize datasets\r\n",
        "classes = unique(y)\r\n",
        "total = len(y)\r\n",
        "classes_y_train = unique(y_train)\r\n",
        "total_y_train = len(y_train)\r\n",
        "imputation=SimpleImputer(strategy='mean')\r\n",
        "scaling=StandardScaler()\r\n",
        "X_train=imputation.fit_transform(X_train)\r\n",
        "X_train=scaling.fit_transform(X_train)\r\n",
        "sm=SMOTE(sampling_strategy=0.4)\r\n",
        "X_train,y_train=sm.fit_resample(X_train,y_train)\r\n",
        "X_test=imputation.transform(X_test)\r\n",
        "X_test=scaling.transform(X_test)\r\n",
        "for c in classes_y_train:\r\n",
        "\tn_examples = len(y_train[y_train==c])\r\n",
        "\tpercent = n_examples / total_y_train * 100\r\n",
        "\tprint('> Class=%d in train: %d/%d (%.1f%%)' % (c, n_examples, total_y_train, percent))\r\n",
        "classes_y_test = unique(y_test)\r\n",
        "total_y_test = len(y_test)\r\n",
        "for c in classes_y_test:\r\n",
        "\tn_examples = len(y_test[y_test==c])\r\n",
        "\tpercent = n_examples / total_y_test * 100\r\n",
        "\tprint('> Class=%d in test: %d/%d (%.1f%%)' % (c, n_examples, total_y_test, percent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qjnRi8yiM2Y"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=110,max_depth=25,max_features='sqrt')\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_RF = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of Random Forest')\r\n",
        "print(confusion_matrix(y_test, y_pred_RF))\r\n",
        "print(classification_report(y_test, y_pred_RF))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQMIH_19tuZr"
      },
      "source": [
        "model=SVC(C=20, gamma=0.2)\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_svm = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('-----------------------------------------')\r\n",
        "print('Prediction results Of SVM')\r\n",
        "print(confusion_matrix(y_test, y_pred_svm))\r\n",
        "print(classification_report(y_test, y_pred_svm))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4TExmC_0ClM"
      },
      "source": [
        "model=KNeighborsClassifier()\r\n",
        "model.fit(X_train,y_train.ravel())\r\n",
        "y_pred_KNN = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of KNN without outliers')\r\n",
        "print(confusion_matrix(y_test, y_pred_KNN))\r\n",
        "print(classification_report(y_test, y_pred_KNN))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "import mlxtend\r\n",
        "from mlxtend.evaluate import bias_variance_decomp\r\n",
        "# estimate bias and variance\r\n",
        "mse, bias, var = bias_variance_decomp(model, X_train, y_train.ravel(), X_test, y_test.ravel(), loss='mse', num_rounds=20, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('MSE: %.3f' % mse)\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4COKdfJy7QQT"
      },
      "source": [
        "print(__doc__)\r\n",
        "\r\n",
        "from sklearn.model_selection import learning_curve\r\n",
        "\r\n",
        "\r\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\r\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\r\n",
        "    if axes is None:\r\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\r\n",
        "\r\n",
        "    axes[0].set_title(title)\r\n",
        "    if ylim is not None:\r\n",
        "        axes[0].set_ylim(*ylim)\r\n",
        "    axes[0].set_xlabel(\"Training examples\")\r\n",
        "    axes[0].set_ylabel(\"Score\")\r\n",
        "\r\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\r\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\r\n",
        "                       train_sizes=train_sizes,\r\n",
        "                       return_times=True)\r\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\r\n",
        "    train_scores_std = np.std(train_scores, axis=1)\r\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\r\n",
        "    test_scores_std = np.std(test_scores, axis=1)\r\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\r\n",
        "    fit_times_std = np.std(fit_times, axis=1)\r\n",
        "\r\n",
        "    # Plot learning curve\r\n",
        "    axes[0].grid()\r\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\r\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\r\n",
        "                         color=\"r\")\r\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\r\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\r\n",
        "                         color=\"g\")\r\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\r\n",
        "                 label=\"Training score\")\r\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\r\n",
        "                 label=\"Cross-validation score\")\r\n",
        "    axes[0].legend(loc=\"best\")\r\n",
        "\r\n",
        "    # Plot n_samples vs fit_times\r\n",
        "    axes[1].grid()\r\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\r\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\r\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\r\n",
        "    axes[1].set_xlabel(\"Training examples\")\r\n",
        "    axes[1].set_ylabel(\"fit_times\")\r\n",
        "    axes[1].set_title(\"Scalability of the model\")\r\n",
        "\r\n",
        "    # Plot fit_time vs score\r\n",
        "    axes[2].grid()\r\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\r\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\r\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\r\n",
        "    axes[2].set_xlabel(\"fit_times\")\r\n",
        "    axes[2].set_ylabel(\"Score\")\r\n",
        "    axes[2].set_title(\"Performance of the model\")\r\n",
        "\r\n",
        "    return plt\r\n",
        "\r\n",
        "\r\n",
        "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\r\n",
        "\r\n",
        "\r\n",
        "title = \"Learning Curves (KNN)\"\r\n",
        "cv = StratifiedKFold(n_splits=5)\r\n",
        "estimators_KNN = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "                    ('scale', StandardScaler()),\r\n",
        "                    ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "                   ('KNN', KNeighborsClassifier())]\r\n",
        "pipeline_KNN = Pipeline(estimators_KNN)\r\n",
        "plot_learning_curve(pipeline_KNN, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\r\n",
        "                    cv=cv, n_jobs=4)\r\n",
        "\r\n",
        "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.2$,$C=20$)\"\r\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\r\n",
        "cv = StratifiedKFold(n_splits=5)\r\n",
        "estimators_svm = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "                    ('scale', StandardScaler()),\r\n",
        "                    ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "                   ('svc', SVC())]\r\n",
        "estimator = SVC(C=20, gamma=0.2)\r\n",
        "pipeline_svm = Pipeline(estimators_svm)\r\n",
        "plot_learning_curve(pipeline_svm, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\r\n",
        "                    cv=cv, n_jobs=4)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}