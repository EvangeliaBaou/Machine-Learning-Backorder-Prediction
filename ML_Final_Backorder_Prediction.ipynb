{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final-Backorder Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxFjdz1kQffQ",
        "outputId": "b2358b84-3a17-467a-87e9-7bd9aae0e174"
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from numpy import unique\r\n",
        "from imblearn.under_sampling import RandomUnderSampler\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from imblearn.pipeline import Pipeline\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn import metrics\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.model_selection import cross_val_predict\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "# import warnings filter\r\n",
        "from warnings import simplefilter\r\n",
        "# ignore all future warnings\r\n",
        "simplefilter(action='ignore', category=FutureWarning)\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import validation_curve\r\n",
        "import mlxtend\r\n",
        "import pandas as pd\r\n",
        "from sklearn.metrics import precision_recall_curve\r\n",
        "from sklearn.metrics import plot_precision_recall_curve\r\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuPovjCAQnWb"
      },
      "source": [
        "df = pd.read_csv('data.csv') #load dataset\r\n",
        "#map binary variables with 'Yes' as 1 and 'No' as 0 \r\n",
        "df['stop_auto_buy']= df['stop_auto_buy'].map({'Yes':1, 'No':0})\r\n",
        "df['potential_issue']= df['potential_issue'].map({'Yes':1, 'No':0})\r\n",
        "df['oe_constraint']= df['oe_constraint'].map({'Yes':1, 'No':0})\r\n",
        "df['ppap_risk']= df['ppap_risk'].map({'Yes':1, 'No':0})\r\n",
        "df['deck_risk']= df['deck_risk'].map({'Yes':1, 'No':0})\r\n",
        "df['rev_stop']= df['rev_stop'].map({'Yes':1, 'No':0})\r\n",
        "df['went_on_backorder']= df['went_on_backorder'].map({'Yes':1, 'No':0})\r\n",
        "df = df.dropna(how='any',axis=0)  #delete nulls\r\n",
        "\r\n",
        "# pie chart of percentage of two classes\r\n",
        "b = df['went_on_backorder'].value_counts()\r\n",
        "total_values = []\r\n",
        "went_on_backorder = ['Class:0', 'Class:1']\r\n",
        "for n in b:\r\n",
        "    total_values.append(n)\r\n",
        "plt.pie(total_values, labels=went_on_backorder, autopct='%0.f%%', shadow=True,\r\n",
        "        startangle=90)  \r\n",
        "plt.show()\r\n",
        "# Input_ y_Target_Variable. \r\n",
        "y = df['went_on_backorder']\r\n",
        "y=y.to_numpy()\r\n",
        "\r\n",
        "#plot correlation matrix (pearson)\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(13,13))\r\n",
        "corr = df.corr()\r\n",
        "sns.heatmap(corr, vmax=1.0, center=0, fmt='.2f',\r\n",
        "                 square=True, linewidths=.6, annot=True, cbar_kws={\"shrink\": .60})\r\n",
        "\r\n",
        "print('Correlation Matrix (Pearsons Correlation)' )\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#plot correlation matrix (Spearman's)\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(13,13))\r\n",
        "corr = df.corr(method='spearman')\r\n",
        "sns.heatmap(corr, vmax=1.0, center=0, fmt='.2f',\r\n",
        "                 square=True, linewidths=.6, annot=True, cbar_kws={\"shrink\": .60})\r\n",
        "print('Correlation Matrix (Spearman Correlation)' )\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#print percentage per class\r\n",
        "classes = unique(y)\r\n",
        "total = len(y)\r\n",
        "for c in classes:\r\n",
        "\tn_examples = len(y[y==c])\r\n",
        "\tpercent = n_examples / total * 100\r\n",
        "\tprint('> Class=%d: %d/%d (%.1f%%)' % (c, n_examples, total, percent))\r\n",
        "\r\n",
        "del df['went_on_backorder'] #delete target col from df\r\n",
        "\r\n",
        "\r\n",
        "df[df['national_inv'] < 0] = 0 #set negative values of feture egual to zero\r\n",
        "df[df['perf_6_month_avg'] < 0] = np.NaN #set negative values of feture as NaN\r\n",
        "df[df['perf_12_month_avg'] < 0] = np.NaN #set negative values of feture as NaN\r\n",
        "\r\n",
        "# Input_x_Features. \r\n",
        "X= df.to_numpy(dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPgqCsPMQ0-P"
      },
      "source": [
        "#split dataset in train and test\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "# summarize datasets\r\n",
        "classes = unique(y)\r\n",
        "total = len(y)\r\n",
        "classes_y_train = unique(y_train)\r\n",
        "total_y_train = len(y_train)\r\n",
        "for c in classes_y_train:\r\n",
        "\tn_examples = len(y_train[y_train==c])\r\n",
        "\tpercent = n_examples / total_y_train * 100\r\n",
        "\tprint('> Class=%d in train: %d/%d (%.1f%%)' % (c, n_examples, total_y_train, percent))\r\n",
        "classes_y_test = unique(y_test)\r\n",
        "total_y_test = len(y_test)\r\n",
        "for c in classes_y_test:\r\n",
        "\tn_examples = len(y_test[y_test==c])\r\n",
        "\tpercent = n_examples / total_y_test * 100\r\n",
        "\tprint('> Class=%d in test: %d/%d (%.1f%%)' % (c, n_examples, total_y_test, percent))\r\n",
        "imputation=SimpleImputer(strategy='mean')\r\n",
        "scaling=StandardScaler()\r\n",
        "X_train=imputation.fit_transform(X_train)\r\n",
        "X_train=scaling.fit_transform(X_train)\r\n",
        "sm=SMOTE(sampling_strategy=0.4)\r\n",
        "X_train,y_train=sm.fit_resample(X_train,y_train)\r\n",
        "X_test=imputation.transform(X_test)\r\n",
        "X_test=scaling.transform(X_test)\r\n",
        "model=SVC()\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_svm = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('-----------------------------------------')\r\n",
        "print('Prediction results Of SVM')\r\n",
        "print(confusion_matrix(y_test, y_pred_svm))\r\n",
        "print(classification_report(y_test, y_pred_svm))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "print('-----------------------------------------')\r\n",
        "model = RandomForestClassifier()\r\n",
        "model.fit(X_train, y_train.ravel())\r\n",
        "y_pred_RF = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of Random Forest')\r\n",
        "print(confusion_matrix(y_test, y_pred_RF))\r\n",
        "print(classification_report(y_test, y_pred_RF))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)\r\n",
        "print('-----------------------------------------')\r\n",
        "model=KNeighborsClassifier()\r\n",
        "model.fit(X_train,y_train.ravel())\r\n",
        "y_pred_KNN = model.predict(X_test)\r\n",
        "# Evaluate predictions\r\n",
        "print('Prediction results Of KNN')\r\n",
        "print(confusion_matrix(y_test, y_pred_KNN))\r\n",
        "print(classification_report(y_test, y_pred_KNN))\r\n",
        "disp = plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "#high precision relates to a low false positive rate\r\n",
        "#high recall relates to a low false negative rate. \r\n",
        "#plot precision-recall\r\n",
        "disp = plot_precision_recall_curve(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0TDv_BVQ-UT"
      },
      "source": [
        "#Hyparameter tunig for Random Forest\r\n",
        "#split dataset in train and test\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "#define the steps of the pipeline for train-validation dataset\r\n",
        "estimators_RF = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "             ('scale', StandardScaler()),\r\n",
        "             ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "               ('RF', RandomForestClassifier())]\r\n",
        "#define grid params  \r\n",
        "grid_RF = [{'RF__criterion': ['gini','entropy'],\r\n",
        "         'RF__n_estimators': [100,110,120,130],\r\n",
        "         'RF__max_depth':[1,5,10,15,20,25],\r\n",
        "         'RF__max_features':['sqrt','auto']}]\r\n",
        "pipeline_RF = Pipeline(estimators_RF)\r\n",
        "kfold = StratifiedKFold(n_splits=5)\r\n",
        "grid_search_RF = GridSearchCV(pipeline_RF, grid_RF,cv=kfold,scoring='f1')\r\n",
        "grid_search_RF.fit(X_train,y_train.ravel()) \r\n",
        "print('Best parameters for Random Forest')\r\n",
        "print(grid_search_RF.best_params_)\r\n",
        "print('---------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAjpR10mRJnM"
      },
      "source": [
        "#Hyparameter tunig for SVM\r\n",
        "estimators_svm = [('imputer', SimpleImputer(strategy='mean')),\r\n",
        "                    ('scale', StandardScaler()),\r\n",
        "                    ('sm',SMOTE(sampling_strategy=0.4)),\r\n",
        "                   ('svc', SVC())]\r\n",
        "#define grid params  \r\n",
        "grid_svm = [{'svc__kernel': ['rbf'],\r\n",
        "         'svc__gamma': ['auto','scale',0.2],\r\n",
        "         'svc__C':[1,10,20]}]\r\n",
        "pipeline_svm = Pipeline(estimators_svm)\r\n",
        "grid_search_svm = GridSearchCV(pipeline_svm, grid_svm,cv=kfold,scoring='f1')\r\n",
        "grid_search_svm.fit(X_train,y_train.ravel()) \r\n",
        "print('Best parameters for SVM')\r\n",
        "print(grid_search_svm.best_params_)\r\n",
        "print('---------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1gWO4_RfI8"
      },
      "source": [
        "#outlier detection\r\n",
        "imputation=SimpleImputer(strategy='mean')\r\n",
        "scaling=StandardScaler()\r\n",
        "X=imputation.fit_transform(X)\r\n",
        "X=scaling.fit_transform(X)\r\n",
        "z = np.abs(stats.zscore(X))\r\n",
        "# value greater then 3 is mark as outlier\r\n",
        "threshold = 3\r\n",
        "new=np.where(z>3)\r\n",
        "indices = unique(new[0])\r\n",
        "how_many_classes=np.take(y, indices)\r\n",
        "classes_outliers = unique(how_many_classes)\r\n",
        "total_outliers = len(how_many_classes)\r\n",
        "for c in classes_outliers:\r\n",
        "  n_examples = len(how_many_classes[how_many_classes==c])\r\n",
        "  percent = (n_examples / total_outliers) * 100\r\n",
        "  print('> Class=%d has: %d/%d (%.1f%%) outliers' % (c, n_examples, total_outliers, percent))\r\n",
        "X = np.delete(X, indices, axis=0)\r\n",
        "y=np.delete(y, indices, axis=0)\r\n",
        "classes_rem_outliers = unique(y)\r\n",
        "total = len(y)\r\n",
        "for c in classes_rem_outliers:\r\n",
        "  n_examples = len(y[y==c])\r\n",
        "  percent = (n_examples / total) * 100\r\n",
        "  print('> Class=%d after removing outliers: %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}